{
  
    
        "post0": {
            "title": "Calculating the gain for Leaky ReLU",
            "content": "Why does kaiming use a**2 instead of abs(a) in the formula . Let&#39;s provide a long-form answer to the question below posted on fastai forum. . Why is the gain used in Kaiming initialization expressed as $ sqrt{ frac{2}{(1+ a^2)}}$, and not $ sqrt{ frac{2}{(1+ left lvert a right rvert)}}$, as posed in this question on Fastai Forum? Isn&#39;t the simpler $ left lvert a right rvert$ a better choice compared with $a^2$? . . Vanishing and exploding activations . Here&#39;s a simple function that would inform us, when we run 100 iterations, whether the activations explode, vanish or behave normally. . def Activations(scaling_factor, iter): x = torch.randn(512) a = torch.randn(512,512)*scaling_factor for i in range(iter): x = a @ x if x.std() != x.std(): print(&quot;Activations exploded at iteration&quot;, i) return if x.std() == 0: print(&quot;Activations vanished at iteration&quot;, i) return print(&quot;Activations did not explode or vanished&quot;) # adapted from https://github.com/fastai/course-v3/blob/master/nbs/dl2/02b_initializing.ipynb . When we do nothing by setting the value of 1.0 as the scaling factor, the activations exploded at iteration 28. . Activations(scaling_factor = 1, iter = 100) . Activations exploded at iteration 28 . When we set the scaling factor too small at 0.01, the activations vanished at iteration 69. . Activations(scaling_factor = 0.01,iter = 100) . Activations vanished at iteration 69 . With the right scaling factor at $ frac{1}{ sqrt{512}}$, the activations behaved normally, completing the entire 100 iterations. . Activations(scaling_factor = 1/math.sqrt(512),iter = 100) . Activations did not explode or vanished . In fact, it could complete 1000 iterations without exploding or vanishing. . Activations(scaling_factor = 1/math.sqrt(512), iter = 1000) . Activations did not explode or vanished . . Tip: For a more in-depth read, refer to Sylvain Gugger&#8217;s Why you need a good init . Correct scaling for init . So how do you get the correct scaling of $ frac{1}{ sqrt{512}}$ for the case described in the last section? . Let’s consider a linear neuron: . $y = (w_{1} times x_{1}) + (w_{2} times x_{2}) + (w_{3} times x_{3})... + (w_{N} times x_{N})$ . The variance of y or $ sigma^2(y)$ is . $ sigma^2(y) = sigma^2 [(w_{1} times x_{1}) + (w_{2} times x_{2}) + (w_{3} times x_{3})... + (w_{N} times x_{N})] $ . Since variables $(w_{1} times x_{1})$,$(w_{2} times x_{2})$,..$(w_{N} times x_{N})$ are uncorrelated, then the variance of the sum is the sum of the variances: . $ sigma^2(y) = sigma^2(w_{1} times x_{1}) + sigma^2(w_{2} times x_{2}) + sigma^2(w_{3} times x_{3})... + sigma^2(w_{N} times x_{N}) $ . $ sigma^2(y) = sum_{i=1}^{N} sigma^2(w_{i} times x_{i}) $ . As all $w_{i}$ and $x_{i}$ are identically and independently distributed, we can rewrite as . $ sigma^2(y) = N times sigma^2(w times x) $ . Note that $ sigma^2(w times x) = E(w^2) sigma^2(x) + E(x^2) sigma^2(w) + sigma^2(x) sigma^2(w) $ . Since $w_{i}$ and $x_{i}$ are normally distributed with mean 0, $E(w_{i})= E(w_{i}^2) = E(x_{i})= E(x_{i}^2) = 0$. . Hence we can write $ sigma^2(w times x) = sigma^2(w) sigma^2(x) $ . This gives us $ sigma^2(y) = N times sigma^2(w) times sigma^2(x) $ . To keep the variance from neither exploding or vanishing, we need to keep . $N times sigma^2(w) = 1$ so that $ sigma^2(y) = sigma^2(x) $ . Hence we have to implement the scaling such that the variance $ sigma^2(w) = frac{1}{N}$ . and correspondingly the standard deviation is $ sigma(w) = frac{1}{ sqrt{N}}$ . . Tip: For a more in-depth read, refer to Prateek Joshi&#8217;s Understanding Xavier Initialization In Deep Neural Networks. If you want to play with codes to gain some intuition, refer to Sylvain Gugger&#8217;s notebook Why you need a good init under the section &quot;The magic number for scaling&quot;. . ReLU . So far we have not used any non-linear activation function yet. Here is the simple ReLU function, commonly used as the non-linear activation function in deep-learning models. . def ReLU(X): return np.maximum(0,X) . The ReLU function can be &quot;Leaky&quot;. The plots below show how different Leaky ReLU functions affect the distribution of the outputs. The inputs come from a normal distribution of $N(0,1)$. When a, the negative slope, is 0, the negative half of the output disribution is gone. This is also the normal ReLU function. . As a increases, the negative portion of the output distribution also starts to become bigger. . When a is 1, the input distribution is exactly the same as the output distribution. . #collapse_hide def leakly_relu(x, negative_slope): return negative_slope*x if x &lt; 0 else x def create_data(xs, negative_slope): y = [leakly_relu(x,negative_slope) for x in xs] return y rows = 4 cols = 3 fig, ax = plt.subplots(rows,cols,figsize=(22,35)) fig.tight_layout() plt.subplots_adjust(hspace = 0.15) sns.set(font_scale= 1.5,style=&quot;darkgrid&quot;) neg_slopes = [0, 0.25, 0.5, 1] relu_names = [&quot;Normal Relu&quot;, &quot;Leaky Relu&quot;, &quot;Leaky Relu&quot;, &quot;No Relu&quot;] x1 = np.arange(-2.00, 2.00, 0.1) x2 = np.random.normal(0, 1, 5000) for row in range(rows): sns.lineplot(x=x1, y=create_data(x1,neg_slopes[row]) ,color=&#39;blue&#39;,ax = ax[row,0]) ax[row,0].set(xlabel=&#39;X&#39;, ylabel=&#39;Relu(X)&#39;) ax[row,0].set_title(f&#39;{relu_names[row]} (a={neg_slopes[row]})&#39;) ax[row,0].axvline(0, -2.00,2.00,color=&#39;white&#39;, linewidth=5, alpha=.7) ax[row,0].hlines( y = 0, color=&#39;white&#39;, linewidth=5, alpha= 0.7, xmin = -2.00, xmax = 2.00) ax[row,0].set_ylim(bottom=-2.00) sns.distplot(x2, color=&quot;b&quot;,ax = ax[row,1],rug=True, kde= False) ax[row,1].set_ylim(0,420) ax[row,1].set_xlim(-3,3) ax[row,1].set(xlabel=&#39;&#39;, ylabel=&#39;Counts&#39;) ax[row,1].set_title(&#39;Input: X&#39;) sns.distplot(create_data(x2,neg_slopes[row]), kde=False, color=&quot;b&quot;,ax = ax[row,2],rug=True) ax[row,2].set_ylim(0,420) ax[row,2].set_xlim(-3,3) ax[row,2].set(xlabel=&#39;&#39;, ylabel=&#39;Counts&#39;) ax[row,2].set_title(&#39;Output: Relu(X)&#39;) . . Clearly, looking at the plots above, we can see that after going through the different Leaky ReLU functions, the output distributions change in different manners. . The problem with having a different output distribution from the input distribution is that deep-learning models require the initialization to be precise. Otherwise, the activations would either explode or vanish. . Correct init scaling for Leaky ReLU . Before going inot the maths, some intuition: regardless of the value of negative slope, half of the distribution remains positive and the other half negative. The negative half, however, might be squished together as the variance of the negtive half becomes smaller. As such, we need to calculate how much gain we need to compensate this smaller variance. . $ Let F(y)$ = $Leaky _ReLU(y)$ . $F(y) = begin{cases} y &amp; quad text{if } y geq 0 a times y &amp; quad text{if } y &lt; text{0 where a = negative slope of Leaky ReLU} end{cases} $ . Note that $P(y geq 0) = frac{1}{2}$ and $P(y&lt; 0) = frac{1}{2}$ (see Appendix for details) . $ sigma^2(F(y)) = begin{cases} sigma^2(y) &amp; quad text{where } P(y geq 0) = frac{1}{2} sigma^2(a times y) &amp; quad text{where } P(y &lt; 0) = frac{1}{2} end{cases} $ . Since $ sigma^2(ax) = a^2 times sigma^2(x)$, . $ sigma^2(F(y)) = begin{cases} sigma^2(y) &amp; quad text{where } P(y geq 0) = frac{1}{2} a^2 sigma^2( y) &amp; quad text{where } P(y &lt; 0) = frac{1}{2} end{cases} $ . $ sigma^2(F(y)) = frac{1}{2} sigma^2(y) + frac{1}{2}a^2 sigma^2( y) $ . $ sigma^2(F(y)) = frac{1}{2} sigma^2(y)( 1 + a^2) $ . $ sigma^2(F(y)) = frac{( 1 + a^2)}{2} sigma^2(y) $ . Since $ sigma^2(y) = N times sigma^2(w) times sigma^2(x) $ . $ sigma^2(F(y)) = frac{( 1 + a^2)}{2} sigma^2(y) = frac{( 1 + a^2)}{2} N times sigma^2(w) times sigma^2(x)$ . To keep the variance from neither exploding or vanishing, we need to keep $ frac{( 1 + a^2)}{2} times N times sigma^2(w) = 1$ so that $ sigma^2(y) = sigma^2(x) $. . This means that the variance is $ sigma^2(w) = frac{1}{N} frac{2}{( 1 + a^2)}$ and . the standard deviation is $ sigma(w) = sqrt frac{1}{ N} sqrt{ frac{2}{( 1 + a^2)}}$ . We can see from the above that the leaky ReLU gain is indeed $ sqrt{ frac{2}{( 1 + a^2)}}$ . . Tip: For more in-depth read into the maths of initialization, refer to Pierre Ouannes&#8217;s How to initialize deep neural networks? Xavier and Kaiming initialization . Gain in Pytorch documentation . However, so far we have not introduce the non-linear function, Leaky ReLU, into the equation. Since Leaky ReLU affects the output distribtuion, as we have seen from the plots above, the scaling should be different too. Otherwise we would encounter exploding or vanishing activations once more. . When we refer to Pytorch documentation, we see the following: . Fills the input Tensor with values according to the method described in Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification - He, K. et al. (2015), using a normal distribution. Also known as He initialization. The resulting tensor will have values sampled from $N(0,std)$ where . torch.nn.init.kaiming_normal_(tensor, a=0, mode=&#39;fan_in&#39;, nonlinearity=&#39;leaky_relu&#39;) . def kaiming_normal_(tensor, a=0, mode=&#39;fan_in&#39;, nonlinearity=&#39;leaky_relu&#39;): fan = _calculate_correct_fan(tensor, mode) gain = calculate_gain(nonlinearity, a) std = gain / math.sqrt(fan) with torch.no_grad(): return tensor.normal_(0, std) . There is an extra Gain, $ sqrt{ frac{2}{(1+a^2) times fan _in}}$, that needs to be added to the scaling. . . The gain for Leaky Relu is $ sqrt{ frac{2}{(1+a^2)}}$, where a is the negative slope of the Leaky Relu function. Let&#39;s run some simple examples. . def gain_math(a): return math.sqrt(2.0 / (1 + a**2)) . Example 1: a = 0.5 . When a is 0.5, the gain is $ sqrt{ frac{2}{(1+(0.5)^2)}} = sqrt{ 1.6} = 1.2649$ . gain_nn = nn.init.calculate_gain(&#39;leaky_relu&#39;,0.5) gain_nn, gain_math(0.5) . (1.2649110640673518, 1.2649110640673518) . Example 2: a = 0.25 . When the negative slope is 0.25, the gain is $ sqrt{ frac{2}{(1+(0.25)^2)}} = sqrt{ 1.88} = 1.3719 $ . gain_nn = nn.init.calculate_gain(&#39;leaky_relu&#39;,0.25) gain_nn, gain_math(0.25) . (1.3719886811400708, 1.3719886811400708) . Appendix . Why $P(y geq 0) = frac{1}{2} text{ and } P(y&lt; 0) = frac{1}{2}$ . $P(y geq 0)$ . $= P(w times x geq 0)$ . $= P(w geq 0 text{ AND } x geq 0 ) text{ OR } P(w &lt; 0 text{ AND } x &lt; 0 )$ . $= P(w geq 0)P(x geq0) text{ + } P(w &lt; 0)P(x &lt; 0) )$ . $= frac{1}{2}P(x geq0) text{ + } frac{1}{2}P(x &lt; 0) )$ . $= frac{1}{2}$ . $P(y &lt; 0)$ . $= P(w times x &lt; 0)$ . $= P(w &lt; 0 text{ AND } x geq 0 )$ OR $P(w geq 0 text{ AND } x &lt; 0 )$ . $= P(w &lt; 0)P(x geq 0) text{ + } P(w geq 0)P(x &lt; 0) )$ . $= frac{1}{2}P(x geq0) text{ + } frac{1}{2}P(x &lt; 0) )$ . $= frac{1}{2}$ . Reference . Why you need a good init by Sylvain Gugger . | Understanding Xavier Initialization In Deep Neural Networks by Prateek Joshi . | How to initialize deep neural networks? Xavier and Kaiming initialization by Pierre Ouannes . | An easy to use blogging platform, with enhanced support for Jupyter Notebooks by Hamel Husain . | Advice for Better Blog Posts by Rachel Thomas . |",
            "url": "https://qwyeow.github.io/ml_blog/notes/2020/04/01/Leaky-ReLU-Gain.html",
            "relUrl": "/notes/2020/04/01/Leaky-ReLU-Gain.html",
            "date": " • Apr 1, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Entropy",
            "content": "Surprisal . Surprisal: $- frac{1}{log(p)}$ . #collapse_hide sns.set(style=&quot;darkgrid&quot;) probability = np.arange(0.01, 1.00, 0.01) surprisal = -np.log2(probability) df = pd.DataFrame(dict(x=probability,y=surprisal)) ax = sns.lineplot(x=&quot;x&quot;, y=&quot;y&quot;,data=df) ax.set(xlabel=&#39;Probability&#39;, ylabel=&#39;Surprisal&#39;) ax.annotate( &#39;The sun will rise tommorow -not surprising&#39;, xy=(1.0,0), xytext=(0.6, 1),size=&#39;small&#39;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05) ) ax.annotate( &#39;The world will end tommorow - very surprising&#39;, xy=(0,6), xytext=(0.1,6),size=&#39;small&#39;, arrowprops=dict(facecolor=&#39;black&#39;, shrink=0.05) ) plt.show() . . Entropy . Entropy: $- sum_{i=1}^{n} p_i frac{1}{log(p_i)}$ .",
            "url": "https://qwyeow.github.io/ml_blog/notes/2020/03/23/crossentropy.html",
            "relUrl": "/notes/2020/03/23/crossentropy.html",
            "date": " • Mar 23, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://qwyeow.github.io/ml_blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://qwyeow.github.io/ml_blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://qwyeow.github.io/ml_blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}